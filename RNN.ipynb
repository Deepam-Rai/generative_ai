{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0ff9027-407a-4b8e-99db-b955826bbb2b",
   "metadata": {},
   "source": [
    "# Hyperparameters and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fdba8ef-d4df-459b-a29c-3c1256d2f702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "from typing import List\n",
    "import re\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils import get_text\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd3a26-3c59-4b11-963b-e0c12164ce10",
   "metadata": {},
   "source": [
    "# Character-wise tokening\n",
    "Here we will take each character as a unique token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4189470-8fc3-47e0-8007-c8b63d173dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "seq_length = 100\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d28b1f1-78e4-46fc-a960-aa90c8484660",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will create custom `Dataset` class that can be used by dataloader with additional methods for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2149fe7-e23c-4a24-adf1-45ffade98b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDatasetPlus(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class that\n",
    "    1. takes path of data-files,\n",
    "    2. extracts texts\n",
    "    This class itself becomes the Dataset class that can be used for DataLoader.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_paths: List[Path], seq_length: int = 10, batch_size: int = 64):\n",
    "        self.paths: List[Path] = data_paths\n",
    "        self.text: str = get_text(self.paths)\n",
    "        self.tokens = list(set(self.tokenize_text(self.text)))\n",
    "        self.tokens.sort()\n",
    "        self.vocab_size = len(self.tokens)\n",
    "        self.token_to_id = {token: idx for idx, token in enumerate(self.tokens)}\n",
    "        self.id_to_token = {idx: token for idx, token in enumerate(self.tokens)}\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.text_as_token_ids = [self.token_to_id[w] for w in self.tokenize_text(self.text)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_as_token_ids) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.text_as_token_ids[index:index+self.seq_length], dtype=torch.long).to(device),\n",
    "            torch.tensor(self.text_as_token_ids[index+1:index+self.seq_length+1], dtype=torch.long).to(device),\n",
    "        )\n",
    "\n",
    "    def tokenize_text(self, text: str) -> list:\n",
    "        return list(text)\n",
    "\n",
    "    def textify_tokens(self, tokens: list) -> str:\n",
    "        return ''.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a681794-2e20-4aad-91f2-80e20135cc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET:  data\\Shakespeare\n",
      "Vocab size: 65  Corpus size: 1215376Number of batches:18988 \t batch_size = 64 \t seq_length = 100\n"
     ]
    }
   ],
   "source": [
    "data_path = Path('./data/Shakespeare')\n",
    "dataset = TextDatasetPlus([data_path], seq_length=seq_length, batch_size=batch_size)\n",
    "vocab_size = dataset.vocab_size\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "print(\n",
    "    f\"DATASET:  {data_path}\\n\"\n",
    "    f\"Vocab size: {dataset.vocab_size}  Corpus size: {len(dataset.text_as_token_ids)}\"\n",
    "    f\"\\tNumber of batches:{len(dataloader)} \\t batch_size = {batch_size} \\t seq_length = {seq_length}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc179622-30a4-4c88-9427-359f1059b7cd",
   "metadata": {},
   "source": [
    "## Understanding dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51bc9e1-06c8-46e1-bcf0-49a01d72a6fd",
   "metadata": {},
   "source": [
    "For each call to `dataset.__getitem__()` we get input and output of length `seq_length`.  \n",
    "Example: for text, \"To be or not to be\" and `seq_length=12`, it would give:  \n",
    "```python\n",
    "(\n",
    "    ['T','o',' ','b','e',' ','o','r',' ','n','o','t',' ','t','o',' ','b'],\n",
    "    ['o',' ','b','e',' ','o','r',' ','n','o','t',' ','t','o',' ','b','e'],\n",
    ")\n",
    "# except that these would be their corresponding numerical-token-ids instead of character.\n",
    "```\n",
    "\n",
    "A batch from dataloader would have `batch_size` many of above things.  \n",
    "\n",
    "Here we will train the model to predict the next character, given the current character and hidden state.  \n",
    "The input-prediction mapping would look as follows:   \n",
    "```python\n",
    "Input:     ['T','o',' ','b','e',' ','o','r',' ','n','o','t',' ','t','o',' ','b'],\n",
    "             |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\n",
    "Target:    ['o',' ','b','e',' ','o','r',' ','n','o','t',' ','t','o',' ','b','e'],\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb38b26-1c32-420e-8328-d07b1e3296fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-Target token-ids from __getitem__():\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59], device='cuda:0')\n",
      "tensor([47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44, 53,\n",
      "        56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,  1,\n",
      "        44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1, 57,\n",
      "        54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,  6,\n",
      "         1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 37, 53, 59,  1], device='cuda:0')\n",
      "---------------- input  -- shape: torch.Size([100])\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "---------------- target  -- shape: torch.Size([100])\n",
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "# Sample __getitem__()\n",
    "X, Y = dataset.__getitem__(1)\n",
    "print(f\"Input-Target token-ids from __getitem__():\\n{X}\\n{Y}\")\n",
    "print(f\"---------------- input  -- shape: {X.shape}\")\n",
    "print(''.join([dataset.id_to_token[x] for x in X.cpu().tolist()]))\n",
    "print(f\"---------------- target  -- shape: {Y.shape}\")\n",
    "print(''.join([dataset.id_to_token[x] for x in Y.cpu().tolist()]))\n",
    "print(f\"----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb14a05c-22c7-4482-8868-80435d07d275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([64, 100]) \t Target shape: torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "# a batch\n",
    "X, Y = next(iter(dataloader))\n",
    "print(f\"Input shape: {X.shape} \\t Target shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d951712-401f-4c13-a7d5-bf53424282e0",
   "metadata": {},
   "source": [
    "## Model\n",
    "We will make use of GRU layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8fe69c4-12b4-4205-b78f-5106505b7092",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNGen(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.gru_layers = num_layers\n",
    "        self.gru_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim\n",
    "        )\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        x = self.embedding(x)\n",
    "        gru_out, states = self.gru(x, states)\n",
    "        out = self.out(gru_out)\n",
    "        return out, states\n",
    "\n",
    "    def init_state(self, batch_size: int):\n",
    "        \"\"\"\n",
    "        Returns the initial hidden state for gru layer.\n",
    "        \"\"\"\n",
    "        return torch.zeros(self.gru_layers, batch_size, self.gru_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6a09430-ece8-4aac-8b2a-baf33f392c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODEL: RNNGen(\n",
      "  (embedding): Embedding(65, 256)\n",
      "  (gru): GRU(256, 1024, batch_first=True)\n",
      "  (out): Linear(in_features=1024, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "hidden_dim = 1024\n",
    "num_layers = 1\n",
    "dropout = 0.2\n",
    "batch_size = 64\n",
    "seq_length = 100\n",
    "epochs = 30\n",
    "\n",
    "model = RNNGen(vocab_size=vocab_size, embed_dim=embed_dim, hidden_dim=hidden_dim, num_layers=num_layers).to(device)\n",
    "print(f\"\\nMODEL: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971a8c2b-b6d8-42bc-ba95-c1edce42b867",
   "metadata": {},
   "source": [
    "## pre-train performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d24441f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, model, seed: str = None, gen_length=100, eval_mode: bool=True):\n",
    "    if seed is None:\n",
    "        tokens = [random.choice(dataset.tokens)]\n",
    "    else:\n",
    "        tokens = dataset.tokenize_text(seed)\n",
    "    if eval_mode:\n",
    "        model.eval()\n",
    "    state = model.init_state(batch_size=1)\n",
    "    for i in range(0, gen_length):\n",
    "        x = torch.tensor([[dataset.token_to_id[w] for w in tokens[i:]]]).to(device)\n",
    "        y_pred, state = model(x, state)\n",
    "        state = state.detach()\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "        token_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        tokens.append(dataset.id_to_token[token_index])\n",
    "    return dataset.textify_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53735d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kKP\n",
      "E\n",
      "arMslvgICdo\n",
      "Tq jtVmlvb!XZQDoy hL;FtVupcGrrLuUDdOl-FarHoDWWgYXkVy;!unyRxzzz&NAZkWyrMnOVPb3H?T-Ec\n"
     ]
    }
   ],
   "source": [
    "print(predict(dataset, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff59d83",
   "metadata": {},
   "source": [
    "## Creating new model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a54f2d",
   "metadata": {},
   "source": [
    "### Training\n",
    "Loss: `CrossEntropyLoss()` as this can be seen as classification problem:\n",
    "> where (cuurent_character, hidden_state) need to be classified as belonging to (class of one of the unique characters)\n",
    "  \n",
    "Optimizer: `Adam`  \n",
    "\n",
    "We also have option to save the checkpoints and generate sample-text after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7bb61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, max_epochs, sequence_length, checkpoints: bool = True, sample_text=True):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        print(f\"\\nEPOCH : {epoch}\")\n",
    "        state = model.init_state(batch_size=batch_size)\n",
    "        for batch, (x, y) in (pbar := tqdm(enumerate(dataloader))):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, state = model(x, state)\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state = state.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch % 1000 == 0:\n",
    "                pbar.set_description(f'epoch: {epoch}/{max_epochs} \\t batch: {batch} \\t loss: {loss.item()} \\t')\n",
    "        if checkpoints:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                Path(f'./models/checkpoints/{datetime.now().strftime(\"%d_%m_%Y__%H_%M_%S\")}__{epoch}__char')\n",
    "            )\n",
    "        if sample_text:\n",
    "            print(\n",
    "                f\"Generated text:\\n\"\n",
    "                f\"-------------------\\n\"\n",
    "                f\"{predict(dataset, model, eval_mode=False)}\\n\"\n",
    "                f\"-------------------\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a64b9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STARTING TRAINING:\n",
      "\n",
      "EPOCH : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1/30 \t batch: 18000 \t loss: 1.379300594329834 \t: : 18988it [15:18, 20.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "-gravive of all,\n",
      "I sup peace the bore to kill'd,\n",
      "Call who so I being eyes of hight,\n",
      "And then will we \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2/30 \t batch: 18000 \t loss: 1.5849789381027222 \t: : 18988it [14:11, 22.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      ".\n",
      "Alclas! I am on humpest answel then the lack'd his libers,\n",
      "Tommovent blesperate is discomes\n",
      "Unpent \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 3/30 \t batch: 18000 \t loss: 1.1817270517349243 \t: : 18988it [14:05, 22.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "her way\n",
      "deformation; in a school;\n",
      "And will you alter'd, I say.\n",
      "Fere\n",
      "is the worthy fault\n",
      "I am few or g\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 4/30 \t batch: 18000 \t loss: 1.1623046398162842 \t: : 18988it [14:04, 22.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "queansk this arms\n",
      "As I will not, say your hearts,\n",
      "That suage my deed'st not part with blesshrely.\n",
      "\n",
      "BE\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 5/30 \t batch: 18000 \t loss: 1.9458537101745605 \t: : 18988it [14:04, 22.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "&ME:\n",
      "SEBAUTOLY:\n",
      "Happre geores:\n",
      "Thishis my le! LAURS:\n",
      "But is know will Look.\n",
      "\n",
      "LARIN SARRENCE:\n",
      "I not bl\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 6/30 \t batch: 18000 \t loss: 1.9079746007919312 \t: : 18988it [14:02, 22.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      ",\n",
      "What armsen farend me?s,\n",
      "That pe cherpit.\n",
      "VOLUMNNIO:\n",
      "Yen, Pe so my pas.\n",
      "Then meage, dainquit.\n",
      "Why, \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 7/30 \t batch: 18000 \t loss: 1.9395318031311035 \t: : 18988it [14:02, 22.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "vUCESTA:\n",
      "Look uizeve my Cloud not Manghight whereepll,\n",
      "Sitize,\n",
      "BRUCHASIS:\n",
      "I lovers,\n",
      "What hearcarry wh\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 8/30 \t batch: 18000 \t loss: 1.9247877597808838 \t: : 18988it [14:02, 22.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "gc?OMNTESTrung'd.\n",
      "Yemby infysweverckisheirstrm.\n",
      "Mou hang onk, antionds\n",
      "at,\n",
      "Marrn dafurry knful uponde\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 9/30 \t batch: 18000 \t loss: 1.9846082925796509 \t: : 18988it [14:02, 22.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      ",&THYCUTHN:\n",
      "Unow,--have ow, Bupord; trulds vid know, I frowead; know my tr my darrdid Comen their.\n",
      "SI\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 10/30 \t batch: 18000 \t loss: 1.7498091459274292 \t: : 18988it [14:01, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "Q$HAnd's my bad, Edwatozew,\n",
      "Whind.\n",
      "Grown the pitizenting,\n",
      "Go, dides macce:\n",
      "Oure crentild, ife.\n",
      "Urits \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 11/30 \t batch: 18000 \t loss: 1.3973308801651 \t: : 18988it [14:00, 22.58it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "T: His ham\n",
      "Shallath towburn, courtish dear.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "This i' it kisd, he faitted to and like \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 12/30 \t batch: 18000 \t loss: 1.2981277704238892 \t: : 18988it [14:01, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "GHAMIANDA:\n",
      "Yond mooming?\n",
      "Hadst, counse butters, jest wsheap in Barname,\n",
      "My and amender may makes hope\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 13/30 \t batch: 18000 \t loss: 1.2730536460876465 \t: : 18988it [14:01, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "izy:\n",
      "Yet by butt be cand?\n",
      "\n",
      "GRUMIO:\n",
      "Anon!\n",
      "The gods he case of joyful next doot in are here you have hi\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 14/30 \t batch: 18000 \t loss: 1.2160072326660156 \t: : 18988it [14:00, 22.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      ". dides thee to hardness\n",
      "Did pardon'd the tran abhor unged\n",
      "Shall here in sole in your father.\n",
      "\n",
      "POMPEY\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 15/30 \t batch: 18000 \t loss: 1.2180771827697754 \t: : 18988it [14:01, 22.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "ET:\n",
      "Ming him and what you come?\n",
      "\n",
      "ABHORSON:\n",
      "No, hear, as just\n",
      "Murgeneth thanks to the bed.\n",
      "\n",
      "HASTINA:\n",
      "T\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 16/30 \t batch: 18000 \t loss: 1.1997156143188477 \t: : 18988it [14:00, 22.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "UKE VINCENTINENIUS:\n",
      "Let's have but peace thou fierceives\n",
      "With precute my doth before I may be as ther\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 17/30 \t batch: 18000 \t loss: 1.203384518623352 \t: : 18988it [14:00, 22.58it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "t reator,\n",
      "A crew shall be the famous walls,\n",
      "And peace. A prized into the tame most purt\n",
      "Is secretty p\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 18/30 \t batch: 18000 \t loss: 1.195938229560852 \t: : 18988it [14:01, 22.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "KY VI:\n",
      "A banius?\n",
      "\n",
      "Nurse:\n",
      "God kept this hundred bleed?\n",
      "ANGELO:\n",
      "'Tis sunder great king.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 19/30 \t batch: 18000 \t loss: 1.1954779624938965 \t: : 18988it [14:01, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "MBRO:\n",
      "What too?\n",
      "\n",
      "ANTIGONUS:\n",
      "Are you only better:\n",
      "The lists you the briddling him all them Forth\n",
      "Woung\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 20/30 \t batch: 18000 \t loss: 1.1936008930206299 \t: : 18988it [14:01, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "$lll, knew my duct.\n",
      "\n",
      "First Citizen:\n",
      "We had palms:\n",
      "That in with his head to my king,\n",
      "When Rome, to but\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 21/30 \t batch: 18000 \t loss: 1.158156394958496 \t: : 18988it [14:01, 22.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "RIUS:\n",
      "Go, go hond, but born, desperpetire here! Your word\n",
      "That hath bud that man lies himself, are ma\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 22/30 \t batch: 18000 \t loss: 1.1933094263076782 \t: : 18988it [14:01, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "XFFA:\n",
      "This beast the dead-beard,\n",
      "After to cour yourself too of Bolicy and receive\n",
      "tay who see senses \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 23/30 \t batch: 18000 \t loss: 1.1669394969940186 \t: : 18988it [14:00, 22.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "Y?\n",
      "\n",
      "MERCUTIO:\n",
      "That's my mother.\n",
      "\n",
      "GLOUCESTER:\n",
      "Sir, my kneeds us bowe,\n",
      "Respecting fire from his new cou\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 24/30 \t batch: 18000 \t loss: 1.1842360496520996 \t: : 18988it [14:01, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "?RIA:\n",
      "What, hunous in their king?\n",
      "\n",
      "CAMILLO:\n",
      "I will wast dead of Richard in share to hews.\n",
      "Would I sha\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 25/30 \t batch: 18000 \t loss: 1.2100005149841309 \t: : 18988it [14:01, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "PHIIS:\n",
      "The bodiest my bitterns none hand\n",
      "Where none. This is the midle upon.\n",
      "\n",
      "KING HENRY VI:\n",
      "Far asho\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 26/30 \t batch: 18000 \t loss: 1.1597119569778442 \t: : 18988it [14:01, 22.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "DWhod absence,\n",
      "Taught and so quaint up with the enemies our crow,\n",
      "And I'll woe to the bold afternoon;\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 27/30 \t batch: 18000 \t loss: 1.193662166595459 \t: : 18988it [14:02, 22.54it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "RUNUS:\n",
      "This is question\n",
      "There o' their feverforces and down our knees.\n",
      "\n",
      "CAMILLO:\n",
      "Who, holour beed to \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 28/30 \t batch: 18000 \t loss: 1.2368059158325195 \t: : 18988it [14:01, 22.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "Qz\n",
      "Forge firm, God's and I:\n",
      "Her dry dies! he for jest day!\n",
      "\n",
      "BENVOLIO:\n",
      "Tuthody, poor with a borneed'st\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 29/30 \t batch: 18000 \t loss: 1.1921899318695068 \t: : 18988it [14:02, 22.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "DWOS do. Why, alack, what Henry neither was dried;\n",
      "incensmy household! what couls must break.\n",
      "\n",
      "KING R\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 30/30 \t batch: 18000 \t loss: 1.2401832342147827 \t: : 18988it [14:02, 22.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "ax-None.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "But when, then I was his life, and hold\n",
      "Shall have you:\n",
      "This o two will, si\n",
      "-------------------\n",
      "\n",
      "TRAINING COMPLETE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTARTING TRAINING:\")\n",
    "train(dataloader, model, max_epochs=epochs, sequence_length=seq_length)\n",
    "print(\"TRAINING COMPLETE.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c5fe46",
   "metadata": {},
   "source": [
    "### Generation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68ec1ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INNE:\n",
      "Virtue hark you!\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "I shall have found, but thou rest in April and me; you'll this knows, as for your love\n",
      "Than as thou deceive, lest thou slay with these wonder'd both your life to\n"
     ]
    }
   ],
   "source": [
    "generated = predict(\n",
    "    dataset=dataset,\n",
    "    model=model,\n",
    "    seed=None,       # starting token; if None, randomly selects starting token\n",
    "    gen_length=200,  # count of new tokens to generate\n",
    "    eval_mode=True   # set model.eval() for prediction\n",
    ")\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24502982",
   "metadata": {},
   "source": [
    "## Loading trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e82ca2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNGen(vocab_size=vocab_size, embed_dim=embed_dim, hidden_dim=hidden_dim, num_layers=num_layers).to(device)\n",
    "model.load_state_dict(torch.load(Path('./models/checkpoints/27_07_2024__05_48_09__30__char'), weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe1522da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y PERTRUTUS:\n",
      "O, even by hand, who ready was then were accomplain:\n",
      "By heaven be less this damnable there?\n",
      "\n",
      "ARIEL:\n",
      "No, sir, your wive of our shall be to hear mock asmilest ever:\n",
      "Since this fires the youn\n"
     ]
    }
   ],
   "source": [
    "generated = predict(\n",
    "    dataset=dataset,\n",
    "    model=model,\n",
    "    seed=None,       # starting token; if None, randomly selects starting token\n",
    "    gen_length=200,  # count of new tokens to generate\n",
    "    eval_mode=True   # set model.eval() for prediction\n",
    ")\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b568c0",
   "metadata": {},
   "source": [
    "# Word-wise tokening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b410f90",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
