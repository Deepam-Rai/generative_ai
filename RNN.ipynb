{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0ff9027-407a-4b8e-99db-b955826bbb2b",
   "metadata": {},
   "source": [
    "# libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fdba8ef-d4df-459b-a29c-3c1256d2f702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "from typing import List\n",
    "import re\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils import get_text\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd3a26-3c59-4b11-963b-e0c12164ce10",
   "metadata": {},
   "source": [
    "# Character-wise tokening\n",
    "Here we will take each character as a unique token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4189470-8fc3-47e0-8007-c8b63d173dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "seq_length = 100\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d28b1f1-78e4-46fc-a960-aa90c8484660",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will create custom `Dataset` class that can be used by dataloader with additional methods for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2149fe7-e23c-4a24-adf1-45ffade98b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDatasetPlus(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class that\n",
    "    1. takes path of data-files,\n",
    "    2. extracts texts\n",
    "    This class itself becomes the Dataset class that can be used for DataLoader.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_paths: List[Path], seq_length: int = 10, batch_size: int = 64):\n",
    "        self.paths: List[Path] = data_paths\n",
    "        self.text: str = get_text(self.paths)\n",
    "        self.tokens = list(set(self.tokenize_text(self.text)))\n",
    "        self.tokens.sort()\n",
    "        self.vocab_size = len(self.tokens)\n",
    "        self.token_to_id = {token: idx for idx, token in enumerate(self.tokens)}\n",
    "        self.id_to_token = {idx: token for idx, token in enumerate(self.tokens)}\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.text_as_token_ids = [self.token_to_id[w] for w in self.tokenize_text(self.text)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_as_token_ids) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.text_as_token_ids[index:index+self.seq_length], dtype=torch.long).to(device),\n",
    "            torch.tensor(self.text_as_token_ids[index+1:index+self.seq_length+1], dtype=torch.long).to(device),\n",
    "        )\n",
    "\n",
    "    def tokenize_text(self, text: str) -> list:\n",
    "        return list(text)\n",
    "\n",
    "    def textify_tokens(self, tokens: list) -> str:\n",
    "        return ''.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a681794-2e20-4aad-91f2-80e20135cc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET:  data\\Shakespeare\n",
      "Vocab size: 65  Corpus size: 1215376Number of batches:18988 \t batch_size = 64 \t seq_length = 100\n"
     ]
    }
   ],
   "source": [
    "data_path = Path('./data/Shakespeare')\n",
    "dataset = TextDatasetPlus([data_path], seq_length=seq_length, batch_size=batch_size)\n",
    "vocab_size = dataset.vocab_size\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "print(\n",
    "    f\"DATASET:  {data_path}\\n\"\n",
    "    f\"Vocab size: {dataset.vocab_size}  Corpus size: {len(dataset.text_as_token_ids)}\"\n",
    "    f\"\\tNumber of batches:{len(dataloader)} \\t batch_size = {batch_size} \\t seq_length = {seq_length}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc179622-30a4-4c88-9427-359f1059b7cd",
   "metadata": {},
   "source": [
    "## Understanding dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51bc9e1-06c8-46e1-bcf0-49a01d72a6fd",
   "metadata": {},
   "source": [
    "For each call to `dataset.__getitem__()` we get input and output of length `seq_length`.  \n",
    "Example: for text, \"To be or not to be\" and `seq_length=12`, it would give:  \n",
    "```python\n",
    "(\n",
    "    ['T','o',' ','b','e',' ','o','r',' ','n','o','t',' ','t','o',' ','b'],\n",
    "    ['o',' ','b','e',' ','o','r',' ','n','o','t',' ','t','o',' ','b','e'],\n",
    ")\n",
    "# except that these would be their corresponding numerical-token-ids instead of character.\n",
    "```\n",
    "\n",
    "A batch from dataloader would have `batch_size` many of above things.  \n",
    "\n",
    "Here we will train the model to predict the next character, given the current character and hidden state.  \n",
    "The input-prediction mapping would look as follows:   \n",
    "```python\n",
    "Input:     ['T','o',' ','b','e',' ','o','r',' ','n','o','t',' ','t','o',' ','b'],\n",
    "             |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\n",
    "Target:    ['o',' ','b','e',' ','o','r',' ','n','o','t',' ','t','o',' ','b','e'],\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb38b26-1c32-420e-8328-d07b1e3296fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-Target token-ids from __getitem__():\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59], device='cuda:0')\n",
      "tensor([47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44, 53,\n",
      "        56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,  1,\n",
      "        44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1, 57,\n",
      "        54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,  6,\n",
      "         1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 37, 53, 59,  1], device='cuda:0')\n",
      "---------------- input  -- shape: torch.Size([100])\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "---------------- target  -- shape: torch.Size([100])\n",
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "# Sample __getitem__()\n",
    "X, Y = dataset.__getitem__(1)\n",
    "print(f\"Input-Target token-ids from __getitem__():\\n{X}\\n{Y}\")\n",
    "print(f\"---------------- input  -- shape: {X.shape}\")\n",
    "print(''.join([dataset.id_to_token[x] for x in X.cpu().tolist()]))\n",
    "print(f\"---------------- target  -- shape: {Y.shape}\")\n",
    "print(''.join([dataset.id_to_token[x] for x in Y.cpu().tolist()]))\n",
    "print(f\"----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb14a05c-22c7-4482-8868-80435d07d275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([64, 100]) \t Target shape: torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "# a batch\n",
    "X, Y = next(iter(dataloader))\n",
    "print(f\"Input shape: {X.shape} \\t Target shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d951712-401f-4c13-a7d5-bf53424282e0",
   "metadata": {},
   "source": [
    "## Model\n",
    "We will make use of GRU layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8fe69c4-12b4-4205-b78f-5106505b7092",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNGen(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.gru_layers = num_layers\n",
    "        self.gru_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim\n",
    "        )\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        x = self.embedding(x)\n",
    "        gru_out, states = self.gru(x, states)\n",
    "        out = self.out(gru_out)\n",
    "        return out, states\n",
    "\n",
    "    def init_state(self, batch_size: int):\n",
    "        \"\"\"\n",
    "        Returns the initial hidden state for gru layer.\n",
    "        \"\"\"\n",
    "        return torch.zeros(self.gru_layers, batch_size, self.gru_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6a09430-ece8-4aac-8b2a-baf33f392c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODEL: RNNGen(\n",
      "  (embedding): Embedding(65, 256)\n",
      "  (gru): GRU(256, 1024, batch_first=True)\n",
      "  (out): Linear(in_features=1024, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "hidden_dim = 1024\n",
    "num_layers = 1\n",
    "dropout = 0.2\n",
    "batch_size = 64\n",
    "seq_length = 100\n",
    "epochs = 30\n",
    "\n",
    "model = RNNGen(vocab_size=vocab_size, embed_dim=embed_dim, hidden_dim=hidden_dim, num_layers=num_layers).to(device)\n",
    "print(f\"\\nMODEL: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971a8c2b-b6d8-42bc-ba95-c1edce42b867",
   "metadata": {},
   "source": [
    "## pre-train performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d24441f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, model, seed: str = None, gen_length=100, eval_mode: bool=True):\n",
    "    if seed is None:\n",
    "        tokens = [random.choice(dataset.tokens)]\n",
    "    else:\n",
    "        tokens = dataset.tokenize_text(seed)\n",
    "    if eval_mode:\n",
    "        model.eval()\n",
    "    state = model.init_state(batch_size=1)\n",
    "    for i in range(0, gen_length):\n",
    "        x = torch.tensor([[dataset.token_to_id[w] for w in tokens[i:]]]).to(device)\n",
    "        y_pred, state = model(x, state)\n",
    "        state = state.detach()\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "        token_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        tokens.append(dataset.id_to_token[token_index])\n",
    "    return dataset.textify_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53735d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kKP\n",
      "E\n",
      "arMslvgICdo\n",
      "Tq jtVmlvb!XZQDoy hL;FtVupcGrrLuUDdOl-FarHoDWWgYXkVy;!unyRxzzz&NAZkWyrMnOVPb3H?T-Ec\n"
     ]
    }
   ],
   "source": [
    "print(predict(dataset, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff59d83",
   "metadata": {},
   "source": [
    "## Creating new model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a54f2d",
   "metadata": {},
   "source": [
    "### Training\n",
    "Loss: `CrossEntropyLoss()` as this can be seen as classification problem:\n",
    "> where (cuurent_character, hidden_state) need to be classified as belonging to (class of one of the unique characters)\n",
    "  \n",
    "Optimizer: `Adam`  \n",
    "\n",
    "We also have option to save the checkpoints and generate sample-text after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7bb61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, max_epochs, sequence_length, checkpoints: bool = True, sample_text=True):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        print(f\"\\nEPOCH : {epoch}\")\n",
    "        state = model.init_state(batch_size=batch_size)\n",
    "        for batch, (x, y) in (pbar := tqdm(enumerate(dataloader))):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, state = model(x, state)\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state = state.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch % 1000 == 0:\n",
    "                pbar.set_description(f'epoch: {epoch}/{max_epochs} \\t batch: {batch} \\t loss: {loss.item()} \\t')\n",
    "        if checkpoints:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                Path(f'./models/checkpoints/{datetime.now().strftime(\"%d_%m_%Y__%H_%M_%S\")}__{epoch}__char.ckpt')\n",
    "            )\n",
    "        if sample_text:\n",
    "            print(\n",
    "                f\"Generated text:\\n\"\n",
    "                f\"-------------------\\n\"\n",
    "                f\"{predict(dataset, model, eval_mode=False)}\\n\"\n",
    "                f\"-------------------\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a64b9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STARTING TRAINING:\n",
      "\n",
      "EPOCH : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1/30 \t batch: 18000 \t loss: 1.379300594329834 \t: : 18988it [15:18, 20.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "-gravive of all,\n",
      "I sup peace the bore to kill'd,\n",
      "Call who so I being eyes of hight,\n",
      "And then will we \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2/30 \t batch: 18000 \t loss: 1.5849789381027222 \t: : 18988it [14:11, 22.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      ".\n",
      "Alclas! I am on humpest answel then the lack'd his libers,\n",
      "Tommovent blesperate is discomes\n",
      "Unpent \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 3/30 \t batch: 18000 \t loss: 1.1817270517349243 \t: : 18988it [14:05, 22.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "her way\n",
      "deformation; in a school;\n",
      "And will you alter'd, I say.\n",
      "Fere\n",
      "is the worthy fault\n",
      "I am few or g\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 4/30 \t batch: 18000 \t loss: 1.1623046398162842 \t: : 18988it [14:04, 22.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "queansk this arms\n",
      "As I will not, say your hearts,\n",
      "That suage my deed'st not part with blesshrely.\n",
      "\n",
      "BE\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 5/30 \t batch: 18000 \t loss: 1.9458537101745605 \t: : 18988it [14:04, 22.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "&ME:\n",
      "SEBAUTOLY:\n",
      "Happre geores:\n",
      "Thishis my le! LAURS:\n",
      "But is know will Look.\n",
      "\n",
      "LARIN SARRENCE:\n",
      "I not bl\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 6/30 \t batch: 18000 \t loss: 1.9079746007919312 \t: : 18988it [14:02, 22.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      ",\n",
      "What armsen farend me?s,\n",
      "That pe cherpit.\n",
      "VOLUMNNIO:\n",
      "Yen, Pe so my pas.\n",
      "Then meage, dainquit.\n",
      "Why, \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 7/30 \t batch: 18000 \t loss: 1.9395318031311035 \t: : 18988it [14:02, 22.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "vUCESTA:\n",
      "Look uizeve my Cloud not Manghight whereepll,\n",
      "Sitize,\n",
      "BRUCHASIS:\n",
      "I lovers,\n",
      "What hearcarry wh\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 8/30 \t batch: 18000 \t loss: 1.9247877597808838 \t: : 18988it [14:02, 22.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "gc?OMNTESTrung'd.\n",
      "Yemby infysweverckisheirstrm.\n",
      "Mou hang onk, antionds\n",
      "at,\n",
      "Marrn dafurry knful uponde\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 9/30 \t batch: 18000 \t loss: 1.9846082925796509 \t: : 18988it [14:02, 22.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      ",&THYCUTHN:\n",
      "Unow,--have ow, Bupord; trulds vid know, I frowead; know my tr my darrdid Comen their.\n",
      "SI\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 10/30 \t batch: 18000 \t loss: 1.7498091459274292 \t: : 18988it [14:01, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "Q$HAnd's my bad, Edwatozew,\n",
      "Whind.\n",
      "Grown the pitizenting,\n",
      "Go, dides macce:\n",
      "Oure crentild, ife.\n",
      "Urits \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 11/30 \t batch: 18000 \t loss: 1.3973308801651 \t: : 18988it [14:00, 22.58it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "T: His ham\n",
      "Shallath towburn, courtish dear.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "This i' it kisd, he faitted to and like \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 12/30 \t batch: 18000 \t loss: 1.2981277704238892 \t: : 18988it [14:01, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "GHAMIANDA:\n",
      "Yond mooming?\n",
      "Hadst, counse butters, jest wsheap in Barname,\n",
      "My and amender may makes hope\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 13/30 \t batch: 18000 \t loss: 1.2730536460876465 \t: : 18988it [14:01, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "izy:\n",
      "Yet by butt be cand?\n",
      "\n",
      "GRUMIO:\n",
      "Anon!\n",
      "The gods he case of joyful next doot in are here you have hi\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 14/30 \t batch: 18000 \t loss: 1.2160072326660156 \t: : 18988it [14:00, 22.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      ". dides thee to hardness\n",
      "Did pardon'd the tran abhor unged\n",
      "Shall here in sole in your father.\n",
      "\n",
      "POMPEY\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 15/30 \t batch: 18000 \t loss: 1.2180771827697754 \t: : 18988it [14:01, 22.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "ET:\n",
      "Ming him and what you come?\n",
      "\n",
      "ABHORSON:\n",
      "No, hear, as just\n",
      "Murgeneth thanks to the bed.\n",
      "\n",
      "HASTINA:\n",
      "T\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 16/30 \t batch: 18000 \t loss: 1.1997156143188477 \t: : 18988it [14:00, 22.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "UKE VINCENTINENIUS:\n",
      "Let's have but peace thou fierceives\n",
      "With precute my doth before I may be as ther\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 17/30 \t batch: 18000 \t loss: 1.203384518623352 \t: : 18988it [14:00, 22.58it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "t reator,\n",
      "A crew shall be the famous walls,\n",
      "And peace. A prized into the tame most purt\n",
      "Is secretty p\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 18/30 \t batch: 18000 \t loss: 1.195938229560852 \t: : 18988it [14:01, 22.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "KY VI:\n",
      "A banius?\n",
      "\n",
      "Nurse:\n",
      "God kept this hundred bleed?\n",
      "ANGELO:\n",
      "'Tis sunder great king.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 19/30 \t batch: 18000 \t loss: 1.1954779624938965 \t: : 18988it [14:01, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "MBRO:\n",
      "What too?\n",
      "\n",
      "ANTIGONUS:\n",
      "Are you only better:\n",
      "The lists you the briddling him all them Forth\n",
      "Woung\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 20/30 \t batch: 18000 \t loss: 1.1936008930206299 \t: : 18988it [14:01, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "$lll, knew my duct.\n",
      "\n",
      "First Citizen:\n",
      "We had palms:\n",
      "That in with his head to my king,\n",
      "When Rome, to but\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 21/30 \t batch: 18000 \t loss: 1.158156394958496 \t: : 18988it [14:01, 22.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "RIUS:\n",
      "Go, go hond, but born, desperpetire here! Your word\n",
      "That hath bud that man lies himself, are ma\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 22/30 \t batch: 18000 \t loss: 1.1933094263076782 \t: : 18988it [14:01, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "XFFA:\n",
      "This beast the dead-beard,\n",
      "After to cour yourself too of Bolicy and receive\n",
      "tay who see senses \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 23/30 \t batch: 18000 \t loss: 1.1669394969940186 \t: : 18988it [14:00, 22.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "Y?\n",
      "\n",
      "MERCUTIO:\n",
      "That's my mother.\n",
      "\n",
      "GLOUCESTER:\n",
      "Sir, my kneeds us bowe,\n",
      "Respecting fire from his new cou\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 24/30 \t batch: 18000 \t loss: 1.1842360496520996 \t: : 18988it [14:01, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "?RIA:\n",
      "What, hunous in their king?\n",
      "\n",
      "CAMILLO:\n",
      "I will wast dead of Richard in share to hews.\n",
      "Would I sha\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 25/30 \t batch: 18000 \t loss: 1.2100005149841309 \t: : 18988it [14:01, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "PHIIS:\n",
      "The bodiest my bitterns none hand\n",
      "Where none. This is the midle upon.\n",
      "\n",
      "KING HENRY VI:\n",
      "Far asho\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 26/30 \t batch: 18000 \t loss: 1.1597119569778442 \t: : 18988it [14:01, 22.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "DWhod absence,\n",
      "Taught and so quaint up with the enemies our crow,\n",
      "And I'll woe to the bold afternoon;\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 27/30 \t batch: 18000 \t loss: 1.193662166595459 \t: : 18988it [14:02, 22.54it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "RUNUS:\n",
      "This is question\n",
      "There o' their feverforces and down our knees.\n",
      "\n",
      "CAMILLO:\n",
      "Who, holour beed to \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 28/30 \t batch: 18000 \t loss: 1.2368059158325195 \t: : 18988it [14:01, 22.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "Qz\n",
      "Forge firm, God's and I:\n",
      "Her dry dies! he for jest day!\n",
      "\n",
      "BENVOLIO:\n",
      "Tuthody, poor with a borneed'st\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 29/30 \t batch: 18000 \t loss: 1.1921899318695068 \t: : 18988it [14:02, 22.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "DWOS do. Why, alack, what Henry neither was dried;\n",
      "incensmy household! what couls must break.\n",
      "\n",
      "KING R\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 30/30 \t batch: 18000 \t loss: 1.2401832342147827 \t: : 18988it [14:02, 22.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "ax-None.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "But when, then I was his life, and hold\n",
      "Shall have you:\n",
      "This o two will, si\n",
      "-------------------\n",
      "\n",
      "TRAINING COMPLETE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTARTING TRAINING:\")\n",
    "train(dataloader, model, max_epochs=epochs, sequence_length=seq_length)\n",
    "print(\"TRAINING COMPLETE.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c5fe46",
   "metadata": {},
   "source": [
    "### Generation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68ec1ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INNE:\n",
      "Virtue hark you!\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "I shall have found, but thou rest in April and me; you'll this knows, as for your love\n",
      "Than as thou deceive, lest thou slay with these wonder'd both your life to\n"
     ]
    }
   ],
   "source": [
    "generated = predict(\n",
    "    dataset=dataset,\n",
    "    model=model,\n",
    "    seed=None,       # starting token; if None, randomly selects starting token\n",
    "    gen_length=200,  # count of new tokens to generate\n",
    "    eval_mode=True   # set model.eval() for prediction\n",
    ")\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24502982",
   "metadata": {},
   "source": [
    "## Loading trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e82ca2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNGen(vocab_size=vocab_size, embed_dim=embed_dim, hidden_dim=hidden_dim, num_layers=num_layers).to(device)\n",
    "model.load_state_dict(torch.load(Path('./models/checkpoints/27_07_2024__05_48_09__30__char.ckpt'), weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe1522da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y PERTRUTUS:\n",
      "O, even by hand, who ready was then were accomplain:\n",
      "By heaven be less this damnable there?\n",
      "\n",
      "ARIEL:\n",
      "No, sir, your wive of our shall be to hear mock asmilest ever:\n",
      "Since this fires the youn\n"
     ]
    }
   ],
   "source": [
    "generated = predict(\n",
    "    dataset=dataset,\n",
    "    model=model,\n",
    "    seed=None,       # starting token; if None, randomly selects starting token\n",
    "    gen_length=200,  # count of new tokens to generate\n",
    "    eval_mode=True   # set model.eval() for prediction\n",
    ")\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b568c0",
   "metadata": {},
   "source": [
    "# Word-wise tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd522b",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will create custom `Dataset` class that can be used by dataloader with additional methods for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7ea7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDatasetPlus(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class that\n",
    "    1. takes path of data-files,\n",
    "    2. extracts texts\n",
    "    This class itself becomes the Dataset class that can be used for DataLoader.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_paths: List[Path], seq_length: int = 10, batch_size: int = 64):\n",
    "        self.paths: List[Path] = data_paths\n",
    "        self.puncs = {\n",
    "            '\\n': '\\n', '\\t': '\\t', '.': '. ', '-': '-', ',': ', ', '!': '! ', '?': '?' ,\n",
    "            '“': '“', '”': '” ',  '(': '(', '—': '—', ')': ') ', '/': '/', '\\'': '\\'', ';': '; ',\n",
    "            ':': ': ', '‘': '‘', '$': '$'\n",
    "        }\n",
    "        self.text: str = get_text(self.paths)\n",
    "        self.tokens = list(set(self.tokenize_text(self.text)))\n",
    "        self.tokens.sort()\n",
    "        self.vocab_size = len(self.tokens)\n",
    "        self.token_to_id = {token: idx for idx, token in enumerate(self.tokens)}\n",
    "        self.id_to_token = {idx: token for idx, token in enumerate(self.tokens)}\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.text_as_token_ids = [self.token_to_id[w] for w in self.tokenize_text(self.text)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_as_token_ids) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.text_as_token_ids[index:index+self.seq_length], dtype=torch.long).to(device),\n",
    "            torch.tensor(self.text_as_token_ids[index+1:index+self.seq_length+1], dtype=torch.long).to(device),\n",
    "        )\n",
    "\n",
    "    def tokenize_text(self, text: str) -> list:\n",
    "        tokens = self.process_text(text=text)\n",
    "        tokens = tokens.split(' ')\n",
    "        tokens = list(filter(lambda x: x != '', tokens))\n",
    "        return tokens\n",
    "    \n",
    "    def textify_tokens(self, tokens: list) -> str:\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def process_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Inserts space before and after punctuations.\n",
    "        :param text:\n",
    "        :return: cleaned text\n",
    "        \"\"\"\n",
    "        cleaned_text = text\n",
    "        for punc in self.puncs:\n",
    "            cleaned_text = cleaned_text.replace(punc, f' {punc} ')\n",
    "        cleaned_text = re.sub(\"[ ]+\", \" \", cleaned_text)\n",
    "        return cleaned_text\n",
    "    \n",
    "    def postprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Tries to unddo opposite of the processings done on input.\n",
    "        \"\"\"\n",
    "        cleaned_text = text\n",
    "        for punc, pre_punc in self.puncs.items():\n",
    "            cleaned_text = cleaned_text.replace(f' {punc}', pre_punc)\n",
    "        cleaned_text = re.sub(\"[ ]+\", \" \", cleaned_text)\n",
    "        return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c6e2279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET:  data\\Shakespeare\n",
      "Vocab size: 14343  Corpus size: 329579\tNumber of batches:5149 \t batch_size = 64 \t seq_length = 10\n"
     ]
    }
   ],
   "source": [
    "data_path = Path('./data/Shakespeare')\n",
    "seq_length = 10\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "dataset = TextDatasetPlus([data_path], seq_length=seq_length, batch_size=batch_size)\n",
    "vocab_size = dataset.vocab_size\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "print(  \n",
    "    f\"DATASET:  {data_path}\\n\"\n",
    "    f\"Vocab size: {dataset.vocab_size}  Corpus size: {len(dataset.text_as_token_ids)}\"\n",
    "    f\"\\tNumber of batches:{len(dataloader)} \\t batch_size = {batch_size} \\t seq_length = {seq_length}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec95dbc2",
   "metadata": {},
   "source": [
    "## Understanding dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21020290",
   "metadata": {},
   "source": [
    "For each call to `dataset.__getitem__()` we get input and output of length `seq_length`.  \n",
    "Example: for text, \"To be or not to be\" and `seq_length=5`, it would give:  \n",
    "```python\n",
    "(\n",
    "    ['To','be','or', 'not','to'],\n",
    "    ['be','or','not','to', 'be'],\n",
    ")\n",
    "# except that these would be their corresponding numerical-token-ids instead of word.\n",
    "```\n",
    "\n",
    "A batch from dataloader would have `batch_size` many of above things.  \n",
    "\n",
    "Here we will train the model to predict the next character, given the current character and hidden state.  \n",
    "The input-prediction mapping would look as follows:   \n",
    "```python\n",
    "Input:     ['To','be','or', 'not','to'],\n",
    "            |     |    |     |     |\n",
    "Target:    ['be','or','not','to', 'be'],\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e08f54f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-Target token-ids from __getitem__():\n",
      "tensor([  571,    10,     0,   317, 13947, 10580,  3530,  7248,     6,  7777],\n",
      "       device='cuda:0')\n",
      "tensor([   10,     0,   317, 13947, 10580,  3530,  7248,     6,  7777,  9131],\n",
      "       device='cuda:0')\n",
      "---------------- input  -- shape: torch.Size([10])\n",
      "Citizen:\n",
      "Beforeweproceedanyfurther,hear\n",
      "---------------- target  -- shape: torch.Size([10])\n",
      ":\n",
      "Beforeweproceedanyfurther,hearme\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "# Sample __getitem__()\n",
    "X, Y = dataset.__getitem__(1)\n",
    "print(f\"Input-Target token-ids from __getitem__():\\n{X}\\n{Y}\")\n",
    "print(f\"---------------- input  -- shape: {X.shape}\")\n",
    "print(''.join([dataset.id_to_token[x] for x in X.cpu().tolist()]))\n",
    "print(f\"---------------- target  -- shape: {Y.shape}\")\n",
    "print(''.join([dataset.id_to_token[x] for x in Y.cpu().tolist()]))\n",
    "print(f\"----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de2e2d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([64, 10]) \t Target shape: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# a batch\n",
    "X, Y = next(iter(dataloader))\n",
    "print(f\"Input shape: {X.shape} \\t Target shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f8f292",
   "metadata": {},
   "source": [
    "## Model\n",
    "We will make use of GRU layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48c30910",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNGen(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.gru_layers = num_layers\n",
    "        self.gru_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim\n",
    "        )\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        x = self.embedding(x)\n",
    "        gru_out, states = self.gru(x, states)\n",
    "        out = self.out(gru_out)\n",
    "        return out, states\n",
    "\n",
    "    def init_state(self, batch_size: int):\n",
    "        \"\"\"\n",
    "        Returns the initial hidden state for gru layer.\n",
    "        \"\"\"\n",
    "        return torch.zeros(self.gru_layers, batch_size, self.gru_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1dc594e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODEL: RNNGen(\n",
      "  (embedding): Embedding(14343, 256)\n",
      "  (gru): GRU(256, 1024, batch_first=True)\n",
      "  (out): Linear(in_features=1024, out_features=14343, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "hidden_dim = 1024\n",
    "num_layers = 1\n",
    "dropout = 0.2\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "model = RNNGen(vocab_size=vocab_size, embed_dim=embed_dim, hidden_dim=hidden_dim, num_layers=num_layers).to(device)\n",
    "print(f\"\\nMODEL: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd20f63",
   "metadata": {},
   "source": [
    "## pre-train performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6e2bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, model, seed: str = None, gen_length=100, eval_mode: bool=True):\n",
    "    if seed is None:\n",
    "        tokens = [random.choice(dataset.tokens)]\n",
    "    else:\n",
    "        tokens = dataset.tokenize_text(seed)\n",
    "    if eval_mode:\n",
    "        model.eval()\n",
    "    state = model.init_state(batch_size=1)\n",
    "    for i in range(0, gen_length):\n",
    "        x = torch.tensor([[dataset.token_to_id[w] for w in tokens[i:]]]).to(device)\n",
    "        y_pred, state = model(x, state)\n",
    "        state = state.detach()\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "        token_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        tokens.append(dataset.id_to_token[token_index])\n",
    "    return dataset.postprocess_text(dataset.textify_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0428da90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infants boughs confined stumbled crush sourly up Fool foulest suffer computation budding party fortnight Sabbare charter lout recorded supply sounder inherent meats law purple sprightfully breeching respect craveth image meekness Press meeds And FERDINAND clapping Peering substantial smell knowest my Prospero butcherly porringer said obsers margent furze daggers unscarr lingers crying Sabbath view Heaven delightful burthens Falstaff feebleness profaneness brotherhood Warwick calf strengthening Youngling rakes garb glut TRINCULO tapsters between Tarrus occident Kneels obeyed hermitage oaks consorted tyrannical Cicester intelligent trot hinds intelligencing bared unsaluted Wrapp craftily reflecting fright natural forenamed reel prorogued unstanched begetting simile thoroughly bands lip headlong Mutually\n"
     ]
    }
   ],
   "source": [
    "print(predict(dataset, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3273b7b",
   "metadata": {},
   "source": [
    "## Creating new model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e183105",
   "metadata": {},
   "source": [
    "### Training\n",
    "Loss: `CrossEntropyLoss()` as this can be seen as classification problem:\n",
    "> where (cuurent_character, hidden_state) need to be classified as belonging to (class of one of the unique characters)\n",
    "  \n",
    "Optimizer: `Adam`  \n",
    "\n",
    "We also have option to save the checkpoints and generate sample-text after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8b68899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, max_epochs, sequence_length, checkpoints: bool = True, sample_text=True):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        print(f\"\\nEPOCH : {epoch}\")\n",
    "        state = model.init_state(batch_size=batch_size)\n",
    "        for batch, (x, y) in (pbar := tqdm(enumerate(dataloader))):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, state = model(x, state)\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state = state.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch % 1000 == 0:\n",
    "                pbar.set_description(f'epoch: {epoch}/{max_epochs} \\t batch: {batch} \\t loss: {loss.item()} \\t')\n",
    "        if checkpoints:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                Path(f'./models/checkpoints/{datetime.now().strftime(\"%d_%m_%Y__%H_%M_%S\")}__{epoch}__word.ckpt')\n",
    "            )\n",
    "        if sample_text:\n",
    "            print(\n",
    "                f\"Generated text:\\n\"\n",
    "                f\"-------------------\\n\"\n",
    "                f\"{predict(dataset, model, gen_length=50, eval_mode=False)}\\n\"\n",
    "                f\"-------------------\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5886d354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STARTING TRAINING:\n",
      "\n",
      "EPOCH : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1/30 \t batch: 5000 \t loss: 2.555070400238037 \t: : 5149it [02:56, 29.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "plenty until unlike. \n",
      "\n",
      " Nurse: \n",
      " Yes, madam: yet still, ha! What fray?\n",
      " Is' t verdict up Lancaster. \n",
      "\n",
      " PETRUCHIO: \n",
      " You peasant swain! you whoreson malt- horse drudge, that calls for company to\n",
      "\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2/30 \t batch: 5000 \t loss: 2.0460205078125 \t: : 5149it [02:55, 29.31it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "dice RODERIGO me excuse. Thou dost, and call the drunkard husband, \n",
      " We will not be your mistemper' d weapons to this shame, \n",
      " Which copest with Richmond with him! \n",
      "\n",
      " WESTMORELAND: \n",
      " He, my lord. \n",
      "\n",
      " COMINIUS: \n",
      "\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 3/30 \t batch: 5000 \t loss: 1.8232061862945557 \t: : 5149it [02:55, 29.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "wisdoms brother, take\n",
      " The one half- chequed bit with her; \n",
      " For I myself have many was too pack. \n",
      "\n",
      " GLOUCESTER: \n",
      " The selfsame name, but, I call' d defend--\n",
      "\n",
      " First Murderer: \n",
      " I' ll\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 4/30 \t batch: 5000 \t loss: 1.8388116359710693 \t: : 5149it [02:54, 29.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "popular ye blaze of his inclination; from him pluck' d two crutches from my feeble limbs, \n",
      " She came adorned hither like me. Yet, to bite his lip, as a fresh tree\n",
      " And kept low shrubs from winter' s powerful wind a mile\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 5/30 \t batch: 5000 \t loss: 1.7041194438934326 \t: : 5149it [02:55, 29.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "nets; but we have we here deliver, Subscribed by the consuls and patricians, \n",
      " Together with sighs, and therewithal\n",
      " Came to my tent; and every cat and dog, \n",
      " And to transport my desires to the\n",
      " For this poor furniture and mean array\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 6/30 \t batch: 5000 \t loss: 1.7595536708831787 \t: : 5149it [02:55, 29.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "betters adventure my person will draw the\n",
      " To buy whose person I to be ruled?\n",
      " O, ' twas I might thank you as you. '\n",
      "\n",
      " PAULINA: \n",
      " Either forbear, \n",
      " Quit presently the chapel, or resolve you\n",
      " For more amazement\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 7/30 \t batch: 5000 \t loss: 1.8514318466186523 \t: : 5149it [02:55, 29.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "porch o' er! \n",
      "\n",
      " PROSPERO: \n",
      " The fringed curtains of threatening drum, \n",
      " That from the seedness the bare fallow brings\n",
      " To teeming foison, even so: \n",
      " Call you me daughter?\n",
      "\n",
      " QUEEN ELIZABETH: \n",
      " What, sovereign sir, \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 8/30 \t batch: 5000 \t loss: 1.8508449792861938 \t: : 5149it [02:54, 29.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "meets us here? my niece Plantagenet\n",
      " Led in the right portial opinion; \n",
      " Or earl of heaven and true love in' t they wretch\n",
      " Shall remain pay for the glasses where against\n",
      " No: first so roundly in thy head; where thou didst vent\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 9/30 \t batch: 5000 \t loss: 1.7472083568572998 \t: : 5149it [02:54, 29.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "petitioners, speak too highly heapt\n",
      " For truth, for you are hers, and be not at a pair of graves\n",
      " Within the earth; and I think\n",
      " I could weep, madam, come. \n",
      " The gods look' d on heal again age: \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 10/30 \t batch: 5000 \t loss: 1.8626976013183594 \t: : 5149it [02:54, 29.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "errlight; \n",
      " But whiles none down that you\n",
      " And what of him. Sirrah Biondello, \n",
      " Now do I turn your current in a ditch, \n",
      " And Roman Lucrece for her chastity: \n",
      " And, Montague, Montague, for I will not away. \n",
      "\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 11/30 \t batch: 5000 \t loss: 1.8386967182159424 \t: : 5149it [02:54, 29.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "thinking\n",
      " With her disdain safe countrymen. \n",
      " The head of Ragozine for Claudio' s: \n",
      " The offence pardons itself. Dear Isabel, \n",
      " One word, good friend. Lucio\n",
      " Of a DOUGLAS: \n",
      " Shall make much of him: \n",
      " As surfeit is\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 12/30 \t batch: 5000 \t loss: 2.02396821975708 \t: : 5149it [02:54, 29.44it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "rising in your cabin for the mischance of me. \n",
      " Go, fellow, get thee to a couch\n",
      " Softer and sweeter\n",
      " And have the Lord of Westmoreland shall maintain. \n",
      "\n",
      " WARWICK: \n",
      " Why, thou unreverend and unhallow' d friar, \n",
      " Not\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 13/30 \t batch: 5000 \t loss: 1.8368638753890991 \t: : 5149it [02:55, 29.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "spark of life be left in thee no sharper spur thee. \n",
      "\n",
      " QUEEN ELIZABETH: \n",
      " So just is God, to right our assembly. \n",
      "\n",
      " BRUTUS: \n",
      " Mark you mercy, sir?\n",
      "\n",
      " Volsce: \n",
      " It cannot be; for' tis\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 14/30 \t batch: 5000 \t loss: 1.9578825235366821 \t: : 5149it [02:55, 29.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "Dazzle mine howling! ' tis lewd- tongued Warwick\n",
      " Is thither gone, and make up the salt\n",
      " Et all their mortifies; and when my\n",
      "\n",
      " ESCALUS: \n",
      " To this princely presence. \n",
      " Now, simple men I say it is done: \n",
      "\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 15/30 \t batch: 5000 \t loss: 1.9801909923553467 \t: : 5149it [02:55, 29.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "Cotswold will be found false. \n",
      " The most replenish' d and sold me whose heavy middle of yourself, \n",
      " And say, I warrant him. \n",
      "\n",
      " Second Lord: \n",
      " The higher powers forbid! \n",
      "\n",
      " PAULINA: \n",
      " I take off so much grief\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 16/30 \t batch: 5000 \t loss: 1.8968713283538818 \t: : 5149it [02:54, 29.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "Watchman: \n",
      " Sovereign, here lies the County Paris slain himself? say to thyself, thou less than we will disperse ourselves: we must confer. \n",
      "\n",
      " BRUTUS: \n",
      " Caius Marcius?\n",
      "\n",
      " CATESBY: \n",
      " I slew thy father, you wot well where\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 17/30 \t batch: 5000 \t loss: 1.9969819784164429 \t: : 5149it [02:55, 29.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "snown conduct. \n",
      " Or all my shame, \n",
      " That copest with death himself to scape a brawl; \n",
      " And now be here' s grey, \n",
      "-- too rough for your days; I will allegiance\n",
      " Out, alas! \n",
      " I lost it, \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 18/30 \t batch: 5000 \t loss: 2.0585572719573975 \t: : 5149it [02:55, 29.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "citizen, \n",
      " Only for saying he is his wooing, whatsoe all this, but rather follow\n",
      " Our forceful instigation? Our prerogative\n",
      " Calls not your counsels, but our natural ripe, my Lord Northumberland. \n",
      "\n",
      " NORTHUMBERLAND: \n",
      " Then, if you talk of\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 19/30 \t batch: 5000 \t loss: 2.0633037090301514 \t: : 5149it [02:55, 29.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "disjunction to show. Cominius the general; \n",
      " But yet he hath\n",
      " possess' ll be the yielding water' d his father' s fortunes forth of France, but worse than wolves of France, \n",
      " And tamed the king, of my brother Angelo left: \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 20/30 \t batch: 5000 \t loss: 2.279778480529785 \t: : 5149it [02:55, 29.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "feels, \n",
      " Making practise with thee: \n",
      " Be all, ' twixt a small-- angel and that so your heart\n",
      " That you shall stifle in your own face, \n",
      " You. They say, her father counts it dangerous\n",
      " Is that temptation that doth\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 21/30 \t batch: 5000 \t loss: 2.358867645263672 \t: : 5149it [02:57, 29.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "betwitched so often burst to belch it in the harmless blood of pretty Rutland?\n",
      "\n",
      " SICINIUS: \n",
      " Not unlike, Verona?\n",
      "\n",
      " PETRUCHIO: \n",
      " I pray, sir, the time is not yesterday, \n",
      " The citizens are mum and speak\n",
      " And, \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 22/30 \t batch: 5000 \t loss: 2.228696346282959 \t: : 5149it [02:55, 29.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "speech of John: \n",
      " Gaunt, a very Mab your voices have; but more he spoke; and would incense me kiss, and I think there' s one' s force should cease. \n",
      " But if an humble prayer\n",
      " May not be made immortal. \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 23/30 \t batch: 5000 \t loss: 2.417424440383911 \t: : 5149it [02:55, 29.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "Edgar, at some seem humbler after it is: and thy best pleasure in these women, of no carriage\n",
      " Prepare thy brow to frown for' t. \n",
      "\n",
      " ISABELLA: \n",
      " When we rode he to claim, \n",
      " In asking their outrage: \n",
      " And\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 24/30 \t batch: 5000 \t loss: 2.3066577911376953 \t: : 5149it [02:55, 29.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "magnificence-- in' s all sink. \n",
      " Let it be but a dozen of what his weary joints would gladly rise, \n",
      " That, if Warwick, your father prays he cannot tell me once: \n",
      " The wicked sustain some harm. \n",
      "\n",
      " GREY: \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 25/30 \t batch: 5000 \t loss: 2.3906874656677246 \t: : 5149it [02:55, 29.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "overset\n",
      " Thy tempest' s past. \n",
      "\n",
      " KING RICHARD III: \n",
      " Give me my boots, your subject all, as I am choked' d in post to think me an old murderer, \n",
      " So thrive, my lord; but needful conference. \n",
      "\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 26/30 \t batch: 5000 \t loss: 2.440810203552246 \t: : 5149it [02:55, 29.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "palpable device. \n",
      " The duke is full of grief, let' s my heart' s great burthen like; \n",
      " For corn' tis beating to become my dreams. \n",
      " And has himself, he would not from his giving is the father' s hid, \n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 27/30 \t batch: 5000 \t loss: 2.446096897125244 \t: : 5149it [02:55, 29.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "interchanging thrusts and blows, \n",
      " Came to your noble fellow! \n",
      "\n",
      " KING EDWARD IV: \n",
      " Therein thou wrong I take it up and down\n",
      " To whom it be true, ' for these poor suffer it; \n",
      " And here I, I would desire to\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 28/30 \t batch: 5000 \t loss: 2.4156339168548584 \t: : 5149it [02:56, 29.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "tribunes of such a case. \n",
      "\n",
      " Boy! I would wish yourself, and thus just speechant' Do but mistake you\n",
      " I cannot tarry to serve God given him\n",
      "\n",
      " FLORIZEL: \n",
      " Alack?\n",
      "\n",
      " GONZALO: ' lay on every side?\n",
      "\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 29/30 \t batch: 5000 \t loss: 2.736767292022705 \t: : 5149it [02:55, 29.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "offence?\n",
      "\n",
      " KING RICHARD III: \n",
      " Who intercepts my expedition?\n",
      "\n",
      " DUCHESS OF YORK: \n",
      " What, are you married to him: but I will deserve. \n",
      "\n",
      " KING HENRY VI: \n",
      " Ay, when a prince\n",
      " How will I report\n",
      "-------------------\n",
      "\n",
      "\n",
      "EPOCH : 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 30/30 \t batch: 5000 \t loss: 2.5000600814819336 \t: : 5149it [02:55, 29.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "-------------------\n",
      "acknowledged spilt it is\n",
      " in me; and at some known. \n",
      "\n",
      " TROILUS: \n",
      " The gods will I have often heard: such is as we hear, march of many kings, and I can I cheque my eyes would rather had eleven die nobly, \n",
      "-------------------\n",
      "\n",
      "TRAINING COMPLETE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTARTING TRAINING:\")\n",
    "train(dataloader, model, max_epochs=epochs, sequence_length=seq_length)\n",
    "print(\"TRAINING COMPLETE.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d48058",
   "metadata": {},
   "source": [
    "### Generation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e8ae684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leave me stay awhile; \n",
      " And not provoked by him for a\n",
      " The head of Ragozine for Claudio is so true thither--\n",
      " Dull thing, say, I pray thee, \n",
      " When the sun sets our first merriment hath been would so: \n",
      " On, \n",
      " Remember Margaret, obdurate! serious vanity! \n",
      " Mis- shapen chaos lady\n",
      "\n",
      " JOHN OF GAUNT: \n",
      " No, no, \n",
      " Call me befall' n. \n",
      " After a storm, a husband for. \n",
      "\n",
      " BENVOLIO: \n",
      " By giving liberty, I say there is no kingdom; for answer you. \n",
      "\n",
      " CLIFFORD: \n",
      " My liege, and Kent the Guildfords are to- night\n",
      " Have my heart for anger burns out of thine from advance\n",
      " And two sleeves' s stomach. Will cry it: \n",
      " And words forty?\n",
      "\n",
      " Pursuivant: \n",
      " God' s bread and garlic arm' d\n",
      " That e' er the higher their hands, \n",
      " And whet the crown- hearted happier voice. Yes, I agree, \n",
      " Even with\n"
     ]
    }
   ],
   "source": [
    "generated = predict(\n",
    "    dataset=dataset,\n",
    "    model=model,\n",
    "    seed=None,       # starting token; if None, randomly selects starting token\n",
    "    gen_length=200,  # count of new tokens to generate\n",
    "    eval_mode=True   # set model.eval() for prediction\n",
    ")\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de5fc2",
   "metadata": {},
   "source": [
    "## Loading trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd14b711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNGen(vocab_size=vocab_size, embed_dim=embed_dim, hidden_dim=hidden_dim, num_layers=num_layers).to(device)\n",
    "model.load_state_dict(torch.load(Path('./models/checkpoints/28_07_2024__23_44_21__30__word.ckpt'), weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4e63870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaffold, there\n",
      " To closeness and the bettering of him\n",
      " to go see the church! \n",
      " For veins befeen the stony holes to\n",
      " This cannot plead his villain in true, Claudio?\n",
      "\n",
      " ISABELLA: \n",
      " Sir John, awhile you dare\n",
      " Less appear too deep and thank them not to murder. \n",
      "\n",
      " PETRUCHIO: \n",
      " What shall the Duke of Hereford- day; \n",
      " For both are false one unworthy this day' s partner, \n",
      " And then take it as you may between two stock- fishes; you have crafted fair a hope will I: \n",
      " Yea, noise; wrong, \n",
      " I charge thee?\n",
      "\n",
      " Pursuivant: \n",
      " And must have buried. Go, do but lose his hire. \n",
      " Had like an virtuous dowry, Petruchio, speak, you have made you king\n",
      " Be so received in tune. \n",
      "\n",
      " BIANCA: \n",
      " Construe them. \n",
      "\n",
      " HENRY BOLINGBROKE: \n",
      " I have a disease were not lawful chosen?'\n",
      "\n",
      " What sayest thou?\n",
      " Be any means my winding- sheet\n"
     ]
    }
   ],
   "source": [
    "generated = predict(\n",
    "    dataset=dataset,\n",
    "    model=model,\n",
    "    seed=None,       # starting token; if None, randomly selects starting token\n",
    "    gen_length=200,  # count of new tokens to generate\n",
    "    eval_mode=True   # set model.eval() for prediction\n",
    ")\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
